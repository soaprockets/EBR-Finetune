# -*- coding: utf-8 -*-
import os
import logging
import random
import torch
import torch.nn as nn
from datasets import load_dataset
from sentence_transformers import SentenceTransformer, models
from sentence_transformers import (
    SentenceTransformer,
    SentenceTransformerTrainer,
    SentenceTransformerTrainingArguments,
)
from sentence_transformers.evaluation import TripletEvaluator, SimilarityFunction
from sentence_transformers.losses import MultipleNegativesRankingLoss,MultipleNegativesSymmetricRankingLoss
from sentence_transformers.training_args import BatchSamplers
from peft import LoraConfig, TaskType
from transformers import TrainerCallback
import argparse

def generate_negative(dataset_Dict):
    candidate_pool = list(set(dataset_Dict["positive"]))
    negatives = []
    for pos in dataset_Dict["positive"]:
        while True:
            neg = random.choice(candidate_pool)
            if neg != pos:
                break
        negatives.append(neg)
    return negatives

def add_text_noise(text, noise_type="random_delete", noise_prob=0.1):
    """
    ä¸ºæ–‡æœ¬æ·»åŠ å™ªå£°ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ
    
    Args:
        text: è¾“å…¥æ–‡æœ¬
        noise_type: å™ªå£°ç±»å‹ ("random_delete", "random_swap", "char_delete", "none")
        noise_prob: å™ªå£°åº”ç”¨æ¦‚ç‡
    
    Returns:
        æ·»åŠ å™ªå£°åçš„æ–‡æœ¬
    """
    if not text or noise_type == "none" or random.random() > noise_prob:
        return text
    
    if noise_type == "random_delete":
        # éšæœºåˆ é™¤è¯ï¼ˆä¿ç•™è¯­ä¹‰ï¼‰
        words = text.split()
        if len(words) <= 1:
            return text
        num_to_delete = max(1, int(len(words) * 0.05))  # åˆ é™¤çº¦5%çš„è¯
        indices_to_keep = sorted(random.sample(range(len(words)), len(words) - num_to_delete))
        return " ".join([words[i] for i in indices_to_keep])
    
    elif noise_type == "random_swap":
        # éšæœºäº¤æ¢ç›¸é‚»è¯
        words = text.split()
        if len(words) <= 1:
            return text
        for _ in range(int(len(words) * 0.02)):  # äº¤æ¢çº¦2%çš„è¯å¯¹
            if len(words) >= 2:
                idx = random.randint(0, len(words) - 2)
                words[idx], words[idx + 1] = words[idx + 1], words[idx]
        return " ".join(words)
    
    elif noise_type == "char_delete":
        # éšæœºåˆ é™¤å­—ç¬¦ï¼ˆè½»å¾®æ‰°åŠ¨ï¼‰
        chars = list(text)
        if len(chars) <= 2:
            return text
        num_to_delete = max(1, int(len(chars) * 0.01))  # åˆ é™¤çº¦1%çš„å­—ç¬¦
        indices_to_keep = sorted(random.sample(range(len(chars)), len(chars) - num_to_delete))
        return "".join([chars[i] for i in indices_to_keep])
    
    return text

def apply_noise_to_example(example, noise_type="random_delete", noise_prob=0.1, apply_to_fields=None):
    """
    ä¸ºæ•°æ®é›†æ ·æœ¬åº”ç”¨å™ªå£°å¢å¼º
    
    Args:
        example: æ•°æ®é›†æ ·æœ¬
        noise_type: å™ªå£°ç±»å‹
        noise_prob: å™ªå£°åº”ç”¨æ¦‚ç‡
        apply_to_fields: åº”ç”¨å™ªå£°çš„å­—æ®µåˆ—è¡¨ï¼ŒNoneåˆ™é»˜è®¤["anchor", "positive"]
    
    Returns:
        æ·»åŠ å™ªå£°åçš„æ ·æœ¬
    """
    if apply_to_fields is None:
        apply_to_fields = ["anchor", "positive"]
    
    for field in apply_to_fields:
        if field in example:
            if isinstance(example[field], list):
                example[field] = [
                    add_text_noise(str(item), noise_type, noise_prob) for item in example[field]
                ]
            elif isinstance(example[field], str):
                example[field] = add_text_noise(example[field], noise_type, noise_prob)
    
    return example

def add_prompt_to_text(text, prompt="Instruct: Retrieve semantically similar text.\nQuery:"):
    """åœ¨æ–‡æœ¬å‰æ·»åŠ  prompt"""
    if isinstance(text, str) and text.strip():
        return f"{prompt} {text}"
    return text

def add_prompt_to_example(example, prompt="Instruct: Retrieve semantically similar text.\nQuery:"):
    """ä¸ºæ•°æ®é›†æ ·æœ¬æ·»åŠ  prompt
    
    Args:
        example: æ•°æ®é›†æ ·æœ¬å­—å…¸
        prompt: è¦æ·»åŠ çš„ prompt æ–‡æœ¬
    
    Returns:
        æ·»åŠ äº† prompt çš„æ ·æœ¬
    """
    # ä¸ºæ‰€æœ‰æ–‡æœ¬å­—æ®µæ·»åŠ  prompt
    if "anchor" in example:
        example["anchor"] = add_prompt_to_text(example["anchor"], prompt)
    if "positive" in example:
        example["positive"] = add_prompt_to_text(example["positive"], prompt)
    if "negative" in example:
        if isinstance(example["negative"], list):
            example["negative"] = [add_prompt_to_text(neg, prompt) for neg in example["negative"]]
        else:
            example["negative"] = add_prompt_to_text(example["negative"], prompt)
    # å¤„ç†å…¶ä»–å¯èƒ½çš„æ–‡æœ¬å­—æ®µ
    if "query" in example:
        example["query"] = add_prompt_to_text(example["query"], prompt)
    if "text" in example:
        example["text"] = add_prompt_to_text(example["text"], prompt)
    return example

def parse_args():
    parser = argparse.ArgumentParser(description="Sentence Transformer with LoRA Fine-tuning")
    
    # æ¨¡å‹é…ç½®
    parser.add_argument("--model_name", type=str, default="./model/KaLM-embedding-multilingual-mini-instruct-v2.5",
                        help="Pretrained model name or path")
    parser.add_argument("--trust_remote_code", action="store_true", default=True,
                        help="Whether to trust remote code when loading model")
    
    # LoRAé…ç½®
    parser.add_argument("--lora_r", type=int, default=64, #64 bge å¾®è°ƒé»˜è®¤å€¼
                        help="LoRA r parameter")
    parser.add_argument("--lora_alpha", type=int, default=128, #128 bgeå¾®è°ƒé»˜è®¤å€¼
                        help="LoRA alpha parameter")
    parser.add_argument("--lora_dropout", type=float, default=0.1,
                        help="LoRA dropout rate")
    
    # Denseå±‚é…ç½®
    parser.add_argument("--dense_dim1", type=int, default=512,
                        help="First dense layer dimension")
    parser.add_argument("--dense_dim2", type=int, default=256,
                        help="Second dense layer dimension")
    
    # æ•°æ®é…ç½®
    parser.add_argument("--data_path", type=str, default="./train_text/", # ./train_data bgeæ¨¡å‹æœ€ä½³å¾®è°ƒæ•°æ®é›†ï¼› ./train_text qwenæœ€ä½³å¾®è°ƒæ•°æ®é›†
                        help="Path to training data")
    parser.add_argument("--use_prompt", action="store_true", default=True,
                        help="Whether to add prompt to query texts")
    parser.add_argument("--prompt", type=str, default="Instruct: Retrieve semantically similar text.\nQuery:",
                        help="Prompt to prepend to query texts (only used when --use_prompt is set)")
    parser.add_argument("--test_size", type=float, default=0.2,
                        help="Test split ratio")
    parser.add_argument("--seed", type=int, default=12,
                        help="Random seed for data splitting")
    
    # è®­ç»ƒé…ç½®
    parser.add_argument("--output_dir", type=str, default="./saved_model/",
                        help="Output directory for saving models")
    parser.add_argument("--num_epochs", type=int, default=1,
                        help="Number of training epochs")
    parser.add_argument("--train_batch_size", type=int, default=6, #16
                        help="Training batch size per device")
    parser.add_argument("--eval_batch_size", type=int, default=4,
                        help="Evaluation batch size per device")
    parser.add_argument("--learning_rate", type=float, default=2e-4, # 2e-4
                        help="Learning rate")
    parser.add_argument("--weight_decay", type=float, default=0.01,
                        help="Weight decay (L2 regularization coefficient)")
    parser.add_argument("--l1_regularization", type=float, default=0.0,
                        help="L1 regularization coefficient (0.0 to disable)")
    parser.add_argument("--max_grad_norm", type=float, default=1.0,
                        help="Maximum gradient norm for gradient clipping (0.0 to disable)")
    parser.add_argument("--eval_steps", type=int, default=500,
                        help="Evaluation steps")
    parser.add_argument("--save_steps", type=int, default=500,
                        help="Save steps")
    parser.add_argument("--warmup_steps", type=int, default=300,
                        help="Warmup steps")
    parser.add_argument("--logging_steps", type=int, default=100,
                        help="Logging steps")
    
    # æ­£åˆ™åŒ–é…ç½®
    parser.add_argument("--label_smoothing", type=float, default=0.0,
                        help="Label smoothing factor (0.0-1.0, 0.0 to disable)")
    parser.add_argument("--early_stopping_patience", type=int, default=0,
                        help="Early stopping patience (0 to disable, number of eval steps without improvement)")
    parser.add_argument("--early_stopping_threshold", type=float, default=0.0,
                        help="Early stopping threshold (minimum improvement to reset patience)")
    parser.add_argument("--lr_scheduler_type", type=str, default="linear",
                        choices=["linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"],
                        help="Learning rate scheduler type")
    parser.add_argument("--lr_scheduler_kwargs", type=str, default=None,
                        help="Additional kwargs for lr scheduler (JSON string, e.g., '{\"num_cycles\": 2}')")
    parser.add_argument("--fp16", action="store_true", default=True,
                        help="Whether to use fp16")
    parser.add_argument("--bf16", action="store_true", default=False),
    parser.add_argument("--max_seq_length", type=int, default=256)
    
    # å†…å­˜ä¼˜åŒ–é…ç½®ï¼ˆè§£å†³ OOM é—®é¢˜ï¼‰
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1,
                        help="Number of gradient accumulation steps (increase to reduce memory, e.g., 2, 4, 8)")
    parser.add_argument("--gradient_checkpointing", action="store_true", default=False,
                        help="Enable gradient checkpointing to save memory (slower but uses less memory)")
    parser.add_argument("--dataloader_num_workers", type=int, default=0,
                        help="Number of dataloader workers (0 for single process, reduce if OOM)")
    parser.add_argument("--dataloader_pin_memory", action="store_true", default=False,
                        help="Pin memory for dataloader (disable if OOM)")
    
    # å™ªå£°/æ•°æ®å¢å¼ºé…ç½®ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
    parser.add_argument("--noise_enabled", action="store_true", default=False,
                        help="Whether to enable text noise augmentation (default: False, use --noise_enabled to enable)")
    parser.add_argument("--noise_type", type=str, default="random_delete",
                        choices=["random_delete", "random_swap", "char_delete", "none"],
                        help="Type of noise to apply: random_delete, random_swap, char_delete, or none")
    parser.add_argument("--noise_prob", type=float, default=0.1,
                        help="Probability of applying noise to each sample (0.0-1.0)")
    parser.add_argument("--noise_apply_to_fields", type=str, nargs="+", default=["anchor", "positive"],
                        help="Fields to apply noise to (default: anchor positive)")
    
    return parser.parse_args()

def print_trainable_parameters(model):
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Trainable params: {trainable_params} || All params: {total_params} || Trainable %: {100 * trainable_params / total_params:.2f}")

class LossMonitorCallback(TrainerCallback):
    """ç›‘æ§è®­ç»ƒ lossï¼Œæ£€æµ‹å¼‚å¸¸æƒ…å†µ"""
    def __init__(self):
        self.loss_history = []
        self.last_loss = None
        
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs and 'loss' in logs:
            current_loss = logs['loss']
            self.loss_history.append(current_loss)
            
            # æ£€æµ‹ loss å¼‚å¸¸ä¸‹é™
            if self.last_loss is not None:
                loss_change = self.last_loss - current_loss
                loss_change_ratio = loss_change / self.last_loss if self.last_loss > 0 else 0
                
                # å¦‚æœ loss ä¸‹é™è¶…è¿‡ 99% æˆ–æ¥è¿‘ 0
                if current_loss < 0.001 and len(self.loss_history) > 5:
                    print(f'\nâš ï¸  è­¦å‘Š: Loss å·²é™è‡³ {current_loss:.6f}ï¼Œå¯èƒ½å‡ºç°è¿‡æ‹Ÿåˆæˆ–æ•°æ®é—®é¢˜')
                    print(f'   å»ºè®®æ£€æŸ¥: 1) æ•°æ®é‡æ˜¯å¦è¶³å¤Ÿ 2) å­¦ä¹ ç‡æ˜¯å¦è¿‡é«˜ 3) æ•°æ®æ˜¯å¦æœ‰é‡å¤')
                
                # å¦‚æœ loss ä¸‹é™è¿‡å¿«ï¼ˆå•æ­¥ä¸‹é™è¶…è¿‡ 50%ï¼‰
                if loss_change_ratio > 0.5 and self.last_loss > 0.1:
                    print(f'\nâš ï¸  æ³¨æ„: Loss ä¸‹é™è¿‡å¿« ({self.last_loss:.4f} â†’ {current_loss:.4f})')
                    print(f'   å¦‚æœæŒç»­å¿«é€Ÿä¸‹é™ï¼Œå¯èƒ½éœ€è¦é™ä½å­¦ä¹ ç‡æˆ–å¢åŠ æ­£åˆ™åŒ–')
            
            self.last_loss = current_loss

class RegularizedLoss:
    """å¸¦ L1 æ­£åˆ™åŒ–çš„ Loss åŒ…è£…å™¨"""
    def __init__(self, base_loss, model, l1_coef=0.0):
        self.base_loss = base_loss
        self.model = model
        self.l1_coef = l1_coef
    
    def __call__(self, sentence_features, labels):
        loss = self.base_loss(sentence_features, labels)
        
        # æ·»åŠ  L1 æ­£åˆ™åŒ–
        if self.l1_coef > 0:
            l1_reg = 0.0
            for param in self.model.parameters():
                if param.requires_grad:
                    l1_reg += torch.sum(torch.abs(param))
            loss = loss + self.l1_coef * l1_reg
        
        return loss
    
    def __getattr__(self, name):
        return getattr(self.base_loss, name)

class EarlyStoppingCallback(TrainerCallback):
    """Early Stopping å›è°ƒ"""
    def __init__(self, patience=3, threshold=0.0, metric_name="eval_loss"):
        self.patience = patience
        self.threshold = threshold
        self.metric_name = metric_name
        self.best_metric = None
        self.patience_counter = 0
        
    def on_evaluate(self, args, state, control, logs=None, **kwargs):
        if logs and self.metric_name in logs:
            current_metric = logs[self.metric_name]
            
            if self.best_metric is None:
                self.best_metric = current_metric
                self.patience_counter = 0
            else:
                # å¯¹äº lossï¼Œè¶Šå°è¶Šå¥½ï¼›å¯¹äº accuracy ç­‰ï¼Œè¶Šå¤§è¶Šå¥½
                if "loss" in self.metric_name.lower():
                    improvement = self.best_metric - current_metric
                else:
                    improvement = current_metric - self.best_metric
                
                if improvement > self.threshold:
                    self.best_metric = current_metric
                    self.patience_counter = 0
                    print(f'âœ“ æŒ‡æ ‡æ”¹å–„: {self.metric_name} = {current_metric:.6f} (æ”¹å–„ {improvement:.6f})')
                else:
                    self.patience_counter += 1
                    print(f'âš ï¸  æŒ‡æ ‡æœªæ”¹å–„: {self.metric_name} = {current_metric:.6f} (patience: {self.patience_counter}/{self.patience})')
                    
                    if self.patience_counter >= self.patience:
                        print(f'\nğŸ›‘ Early Stopping: {self.patience} æ¬¡è¯„ä¼°æœªæ”¹å–„ï¼Œåœæ­¢è®­ç»ƒ')
                        control.should_training_stop = True

def main():
    args = parse_args()
    
    logging.basicConfig(format="%(asctime)s - %(message)s", datefmt="%Y-%m-%d %H:%M:%S", level=logging.INFO)
    
    print(f'å¼€å§‹åŠ è½½æ¨¡å‹')
    base_model = SentenceTransformer(
        args.model_name,
        # truncate_dim=256
        # trust_remote_code=args.trust_remote_code
    )
    logging.info(base_model)
    
    # åº”ç”¨ LoRA é…ç½®
    lora_config = LoraConfig(
        task_type=TaskType.FEATURE_EXTRACTION,
        # inference_mode=False,
        r=args.lora_r,
        lora_alpha=args.lora_alpha,
        lora_dropout=args.lora_dropout,
        bias="none",
        # target_modules=["Wo", "Wqkv"]
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "down_proj", "gate_proj", "up_proj"]
    )
    base_model.add_adapter(lora_config)
    
    # æ‰“å°å¯è®­ç»ƒå‚æ•°
    print_trainable_parameters(base_model)
    
    # å†…å­˜ä¼˜åŒ–æç¤º
    if args.train_batch_size >= 8 or args.gradient_accumulation_steps == 1:
        print(f'å†…å­˜ä¼˜åŒ–æç¤º: å½“å‰ batch_size={args.train_batch_size}, gradient_accumulation_steps={args.gradient_accumulation_steps}')
        if args.train_batch_size >= 8:
            print('  - å¦‚æœé‡åˆ° OOMï¼Œå»ºè®®å‡å° --train_batch_size (ä¾‹å¦‚: 4, 2, 1)')
        if args.gradient_accumulation_steps == 1:
            print('  - å»ºè®®å¢åŠ  --gradient_accumulation_steps (ä¾‹å¦‚: 2, 4, 8) æ¥ä¿æŒæœ‰æ•ˆ batch size')
        if not args.gradient_checkpointing:
            print('  - å¯ä»¥å¯ç”¨ --gradient_checkpointing æ¥èŠ‚çœå†…å­˜ï¼ˆè®­ç»ƒä¼šç¨æ…¢ï¼‰')
    
    # dense_layer1 = models.Dense(
    #     in_features=base_model.get_sentence_embedding_dimension(),
    #     out_features=args.dense_dim1,
    #     activation_function=nn.SiLU())
    # dense_layer2 = models.Dense(
        # in_features=base_model.get_sentence_embedding_dimension(),
        # out_features=args.dense_dim2,
        # activation_function=nn.GELU())
    # model = SentenceTransformer(modules=[base_model,dense_layer2])

    model = base_model
    
    model_name_only = args.model_name.split('/')[-1]
    print(f'æ¨¡å‹åŠ è½½å®Œæˆ')
    
    print(f'å¼€å§‹åŠ è½½æ•°æ®')
    json_files = [os.path.join(args.data_path, f) for f in os.listdir(args.data_path)]
    dataset = load_dataset('json', data_files=json_files)
    dataset = dataset.filter(lambda example: example != '')
    
    # æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦æ·»åŠ  prompt åˆ°æ‰€æœ‰æ–‡æœ¬å­—æ®µ
    if args.use_prompt:
        print(f'ä¸ºæ•°æ®æ·»åŠ  prompt: "{args.prompt}"')
        dataset = dataset.map(
            lambda x: add_prompt_to_example(x, args.prompt), 
            desc="Adding prompt to texts"
        )
        print(f'æ•°æ®åŠ è½½å®Œæˆï¼Œå·²ä¸ºæ‰€æœ‰æŸ¥è¯¢æ–‡æœ¬æ·»åŠ  prompt')
    else:
        print(f'æ•°æ®åŠ è½½å®Œæˆï¼Œæœªæ·»åŠ  prompt')
    
    dataset_dict = dataset['train'].train_test_split(test_size=args.test_size, seed=args.seed)
    train_dataset = dataset_dict['train']
    eval_dataset = dataset_dict['test']
    
    # æ‰“å°æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
    print(f'è®­ç»ƒé›†å¤§å°: {len(train_dataset)}, éªŒè¯é›†å¤§å°: {len(eval_dataset)}')
    if len(train_dataset) < 100:
        print(f'âš ï¸  è­¦å‘Š: è®­ç»ƒé›†æ ·æœ¬æ•°è¾ƒå°‘ ({len(train_dataset)})ï¼Œå¯èƒ½å¯¼è‡´å¿«é€Ÿè¿‡æ‹Ÿåˆ')
    if len(train_dataset) < 10:
        print(f'âš ï¸  ä¸¥é‡è­¦å‘Š: è®­ç»ƒé›†æ ·æœ¬æ•°è¿‡å°‘ ({len(train_dataset)})ï¼Œå»ºè®®å¢åŠ æ•°æ®é‡')
    
    # åº”ç”¨æ–‡æœ¬å™ªå£°å¢å¼ºï¼ˆä»…å¯¹è®­ç»ƒé›†ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
    if args.noise_enabled:
        noise_fields = args.noise_apply_to_fields
        print(f'åº”ç”¨æ–‡æœ¬å™ªå£°å¢å¼º: type={args.noise_type}, prob={args.noise_prob}, fields={noise_fields}')
        train_dataset = train_dataset.map(
            lambda x: apply_noise_to_example(x, args.noise_type, args.noise_prob, noise_fields),
            desc="Applying noise augmentation to training data"
        )
    
    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœå†…å­˜ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if args.gradient_checkpointing:
        if hasattr(base_model, 'enable_input_require_grads'):
            base_model.enable_input_require_grads()
        if hasattr(base_model[0], 'gradient_checkpointing_enable'):
            base_model[0].gradient_checkpointing_enable()
            print('å·²å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ (Gradient Checkpointing) ä»¥èŠ‚çœå†…å­˜')
    
    # åˆ›å»ºåŸºç¡€ loss
    base_loss = MultipleNegativesSymmetricRankingLoss(model)
    
    # åº”ç”¨ L1 æ­£åˆ™åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    # åˆ›å»ºåŸºç¡€ loss
    if args.l1_regularization > 0:
        # å®šä¹‰ä¸€ä¸ªå·¥å‚å‡½æ•°ï¼Œæ¥æ”¶ model å‚æ•°å¹¶è¿”å› RegularizedLoss å®ä¾‹
        def create_loss(model):
            base_loss = MultipleNegativesSymmetricRankingLoss(model)
            return RegularizedLoss(base_loss, model, args.l1_regularization)
        loss = create_loss  # æ³¨æ„ï¼šè¿™é‡Œä¼ é€’çš„æ˜¯å‡½æ•°ï¼Œä¸æ˜¯å®ä¾‹
        print(f'âœ“ å·²å¯ç”¨ L1 æ­£åˆ™åŒ–: coefficient = {args.l1_regularization}')
    else:
        loss = MultipleNegativesSymmetricRankingLoss  # ä¼ é€’ç±»æœ¬èº«ï¼Œä¸æ˜¯å®ä¾‹
    
    # æ‰“å°æ­£åˆ™åŒ–é…ç½®
    print(f'\næ­£åˆ™åŒ–é…ç½®:')
    print(f'  - Weight Decay (L2): {args.weight_decay}')
    print(f'  - L1 Regularization: {args.l1_regularization if args.l1_regularization > 0 else "ç¦ç”¨"}')
    print(f'  - LoRA Dropout: {args.lora_dropout}')
    print(f'  - Max Grad Norm: {args.max_grad_norm if args.max_grad_norm > 0 else "ç¦ç”¨"}')
    print(f'  - Label Smoothing: {args.label_smoothing if args.label_smoothing > 0 else "ç¦ç”¨"}')
    if args.early_stopping_patience > 0:
        print(f'  - Early Stopping: patience={args.early_stopping_patience}, threshold={args.early_stopping_threshold}')
    print(f'  - LR Scheduler: {args.lr_scheduler_type}')
    
    run_name = f'{model_name_only}-peft-lora'
    
    # è§£æ lr_scheduler_kwargs
    lr_scheduler_kwargs = {}
    if args.lr_scheduler_kwargs:
        import json
        try:
            lr_scheduler_kwargs = json.loads(args.lr_scheduler_kwargs)
        except:
            print(f'âš ï¸  è­¦å‘Š: æ— æ³•è§£æ lr_scheduler_kwargsï¼Œä½¿ç”¨é»˜è®¤å€¼')
    
    training_args = SentenceTransformerTrainingArguments(
        output_dir=os.path.join(args.output_dir, run_name),
        num_train_epochs=args.num_epochs,
        per_device_train_batch_size=args.train_batch_size,
        per_device_eval_batch_size=args.eval_batch_size,
        learning_rate=args.learning_rate,
        weight_decay=args.weight_decay,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        max_grad_norm=args.max_grad_norm if args.max_grad_norm > 0 else None,
        batch_sampler=BatchSamplers.NO_DUPLICATES,
        eval_strategy="steps",
        eval_steps=args.eval_steps,
        save_strategy='steps',
        save_steps=args.save_steps,
        save_total_limit=3,
        warmup_steps=args.warmup_steps,
        logging_steps=args.logging_steps,
        logging_dir=os.path.join(args.output_dir, run_name),
        fp16=args.fp16,
        bf16=args.bf16,
        dataloader_num_workers=args.dataloader_num_workers,
        dataloader_pin_memory=args.dataloader_pin_memory,
        lr_scheduler_type=args.lr_scheduler_type,
        **lr_scheduler_kwargs
    )
    
    dev_evaluator = TripletEvaluator(
        anchors=eval_dataset["anchor"],
        positives=eval_dataset["positive"],
        negatives=generate_negative(eval_dataset),
        main_similarity_function=SimilarityFunction.COSINE,
        name="sts-dev"
    )
    
    print(f'å¼€å§‹è®­ç»ƒ')
    # æ·»åŠ è®­ç»ƒç›‘æ§å›è°ƒ
    callbacks = [LossMonitorCallback()]
    
    # æ·»åŠ  Early Stopping å›è°ƒï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if args.early_stopping_patience > 0:
        early_stopping = EarlyStoppingCallback(
            patience=args.early_stopping_patience,
            threshold=args.early_stopping_threshold,
            metric_name="eval_loss"
        )
        callbacks.append(early_stopping)
        print(f'âœ“ å·²å¯ç”¨ Early Stopping: patience={args.early_stopping_patience}, threshold={args.early_stopping_threshold}')
    
    trainer = SentenceTransformerTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        loss=loss,
        evaluator=[dev_evaluator]
        # max_seq_length=args.max_seq_length
    )
    
    # æ·»åŠ å›è°ƒï¼ˆå¦‚æœæ”¯æŒï¼‰
    for callback in callbacks:
        try:
            trainer.add_callback(callback)
        except:
            # å¦‚æœä¸æ”¯æŒ add_callbackï¼Œå°è¯•é€šè¿‡ callbacks å‚æ•°
            try:
                trainer.callback_handler.add_callback(callback)
            except:
                print(f'æ³¨æ„: æ— æ³•æ·»åŠ å›è°ƒ {type(callback).__name__}ï¼Œä½†è®­ç»ƒä¼šç»§ç»­')
    
    trainer.train()
    print(f'è®­ç»ƒå®Œæˆ')
    
    print(f'å¼€å§‹ä¿å­˜æ¨¡å‹')
    final_output_dir = os.path.join(args.output_dir, run_name)
    model.save_pretrained(final_output_dir)
    print(f'æ¨¡å‹ä¿å­˜å®Œæˆ')

if __name__ == "__main__":
    main()